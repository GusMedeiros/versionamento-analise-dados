{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:14:07.399743Z",
     "start_time": "2025-06-27T02:13:41.701804Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando varredura em 'repositories-mined'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando repositórios:  31%|███       | 4/13 [04:54<11:02, 73.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m caminho_arquivo \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_path, arquivo_json)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcaminho_arquivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     60\u001b[0m         pr \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Adiciona informações extra que serão úteis na análise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm # Adicionado para melhor feedback visual\n",
    "\n",
    "# --- 1. Configuração e Carregamento de Dados ---\n",
    "# Caminho base\n",
    "base_path = \"repositories-mined\"\n",
    "\n",
    "# Armazenamento dos dados de PRs\n",
    "todos_os_prs = []\n",
    "\n",
    "print(f\"Iniciando varredura em '{base_path}'...\")\n",
    "\n",
    "# Lista de repositórios para iterar com uma barra de progresso\n",
    "repo_list = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "# Percorre cada repositório\n",
    "for repo in tqdm(repo_list, desc=\"Processando repositórios\"):\n",
    "    repo_path = os.path.join(base_path, repo)\n",
    "\n",
    "    # Lê os autores do arquivo de amostra\n",
    "    sample_path = os.path.join(repo_path, \"sample-devs.jsonl\")\n",
    "    if not os.path.isfile(sample_path):\n",
    "        continue\n",
    "\n",
    "    # Carrega os desenvolvedores da amostra para este repositório\n",
    "    devs_neste_repo = {}\n",
    "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            dev_data = json.loads(line)\n",
    "            devs_neste_repo[dev_data['author'].lower()] = dev_data['faixa']\n",
    "\n",
    "    # Percorre a pasta de desenvolvedores para encontrar os PRs\n",
    "    devs_folder_path = os.path.join(repo_path, \"developer\")\n",
    "    if not os.path.isdir(devs_folder_path):\n",
    "        continue\n",
    "\n",
    "    for dev_name in os.listdir(devs_folder_path):\n",
    "        # Verifica se este desenvolvedor está na nossa amostra\n",
    "        if dev_name.lower() in devs_neste_repo:\n",
    "            faixa = devs_neste_repo[dev_name.lower()]\n",
    "            results_path = os.path.join(devs_folder_path, dev_name, \"results\")\n",
    "\n",
    "            if not os.path.isdir(results_path):\n",
    "                continue\n",
    "\n",
    "            for arquivo_json in os.listdir(results_path):\n",
    "                if not arquivo_json.endswith(\".json\"):\n",
    "                    continue\n",
    "\n",
    "                caminho_arquivo = os.path.join(results_path, arquivo_json)\n",
    "\n",
    "                try:\n",
    "                    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "                        pr = json.load(f)\n",
    "\n",
    "                    # Adiciona informações extra que serão úteis na análise\n",
    "                    pr['author'] = dev_name\n",
    "                    pr['faixa'] = faixa\n",
    "                    pr['repo'] = repo\n",
    "                    todos_os_prs.append(pr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Aviso: Erro ao ler o arquivo {caminho_arquivo}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal de PRs coletados: {len(todos_os_prs)}\")\n",
    "\n",
    "# --- 2. Análise de Eficiência e Processamento de Dados ---\n",
    "def analisar_eficiencia(prs):\n",
    "    \"\"\"\n",
    "    Processa a lista de PRs brutos e extrai métricas de eficiência.\n",
    "    \"\"\"\n",
    "    registros = []\n",
    "\n",
    "    for pr in prs:\n",
    "        try:\n",
    "            # Converte as strings de data para objetos datetime\n",
    "            created_at = datetime.fromisoformat(pr['created_at'].replace(\"Z\", \"+00:00\"))\n",
    "        except (ValueError, TypeError):\n",
    "            continue # Pula o PR se a data de criação for inválida\n",
    "\n",
    "        merged_at = pr.get(\"merged_at\")\n",
    "        tempo_merge = None\n",
    "        if merged_at:\n",
    "            try:\n",
    "                merged = datetime.fromisoformat(merged_at.replace(\"Z\", \"+00:00\"))\n",
    "                # Calcula a diferença em horas\n",
    "                tempo_merge = (merged - created_at).total_seconds() / 3600\n",
    "            except (ValueError, TypeError):\n",
    "                pass # Ignora se a data de merge for inválida\n",
    "\n",
    "        # Processa o status da Integração Contínua (CI)\n",
    "        ci_status = pr.get(\"ci_status_on_head\", \"unknown\")\n",
    "        ci_sucesso = 1 if ci_status == \"success\" else 0\n",
    "        # Considera um status de CI válido se for um dos três esperados\n",
    "        ci_valido = ci_status in [\"success\", \"failure\", \"pending\"]\n",
    "\n",
    "        registros.append({\n",
    "            \"faixa\": pr[\"faixa\"],\n",
    "            \"tempo_merge_horas\": tempo_merge,\n",
    "            \"ci_status\": ci_status,\n",
    "            \"ci_sucesso\": ci_sucesso,\n",
    "            \"ci_valido\": ci_valido,\n",
    "            \"hora_criacao\": created_at.hour,\n",
    "            \"dia_semana_criacao\": created_at.weekday(), # 0=Segunda, 6=Domingo\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(registros)\n",
    "\n",
    "df = analisar_eficiencia(todos_os_prs)\n",
    "\n",
    "# --- 3. Configuração de Visualização ---\n",
    "# Paleta de cores consistente para garantir que a Faixa A sempre tenha a mesma cor, etc.\n",
    "ordem_faixas_original = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "ordem_graficos = [\"E\", \"D\", \"C\", \"B\", \"A\"] # Ordem invertida para visualização\n",
    "\n",
    "cores_paleta = sns.color_palette(\"husl\", 8)\n",
    "paleta_consistente = dict(zip(ordem_faixas_original, cores_paleta))\n",
    "\n",
    "# Define um estilo global para os gráficos\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1, rc={\"figure.figsize\": (12, 7)})\n",
    "\n",
    "\n",
    "# --- 4. Geração e Exibição de Gráficos ---\n",
    "def gerar_graficos(df):\n",
    "    \"\"\"\n",
    "    Gera e exibe os gráficos da análise de eficiência diretamente no notebook.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Gerando Gráficos de Análise de Eficiência ---\")\n",
    "\n",
    "    # --- Gráfico 1: Tempo médio para merge ---\n",
    "    plt.figure() # Cria uma nova figura para o gráfico\n",
    "    sns.barplot(data=df, x='faixa', y='tempo_merge_horas',\n",
    "                order=ordem_graficos, errorbar='sd', palette=paleta_consistente, capsize=.2)\n",
    "    plt.title(\"Tempo Médio para Merge por Faixa (com Desvio Padrão)\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Tempo para Merge (em horas)\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Gráfico 2: Taxa de sucesso da CI ---\n",
    "    plt.figure()\n",
    "    # Calcula a média de sucesso (1s e 0s) para cada faixa\n",
    "    df_ci = df[df[\"ci_valido\"]].groupby(\"faixa\")[\"ci_sucesso\"].mean().reindex(ordem_graficos)\n",
    "    sns.barplot(x=df_ci.index, y=df_ci.values, palette=paleta_consistente)\n",
    "    plt.title(\"Taxa de Sucesso da Integração Contínua (CI) por Faixa\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Taxa de Sucesso (%)\", fontsize=12)\n",
    "    plt.ylim(0, 1) # Define o eixo Y de 0 a 100%\n",
    "    # Formata o eixo Y para mostrar porcentagens\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    plt.show()\n",
    "\n",
    "    # --- Gráfico 3: Distribuição da Hora de criação ---\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=df, x='faixa', y='hora_criacao', order=ordem_graficos, palette=paleta_consistente)\n",
    "    plt.title(\"Distribuição da Hora de Criação de PRs por Faixa\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Hora do Dia (0-23h)\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Gráfico 4: Distribuição do Dia da semana de criação ---\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=df, x='faixa', y='dia_semana_criacao', order=ordem_graficos, palette=paleta_consistente)\n",
    "    plt.title(\"Distribuição do Dia da Semana de Criação de PRs por Faixa\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Dia da Semana\", fontsize=12)\n",
    "    plt.yticks(ticks=range(7), labels=[\"Segunda\", \"Terça\", \"Quarta\", \"Quinta\", \"Sexta\", \"Sábado\", \"Domingo\"])\n",
    "    plt.show()\n",
    "\n",
    "# --- 5. Execução e Exibição de Resumos ---\n",
    "if not df.empty:\n",
    "    # Exibir resumo no console\n",
    "    print(\"\\n\\n--- Resumo dos Dados ---\")\n",
    "    print(\"\\n--- Tempo Médio para Merge por Faixa (em horas) ---\")\n",
    "    print(df.groupby('faixa')['tempo_merge_horas'].mean().reindex(ordem_graficos).round(2))\n",
    "\n",
    "    print(\"\\n--- Taxa de Sucesso da CI por Faixa ---\")\n",
    "    print(df[df[\"ci_valido\"]].groupby('faixa')['ci_sucesso'].mean().reindex(ordem_graficos).round(3))\n",
    "\n",
    "    print(\"\\n--- Hora Média de Criação ---\")\n",
    "    print(df.groupby('faixa')['hora_criacao'].mean().reindex(ordem_graficos).round(2))\n",
    "\n",
    "    print(\"\\n--- Dia Médio de Criação (0=Segunda) ---\")\n",
    "    print(df.groupby('faixa')['dia_semana_criacao'].mean().reindex(ordem_graficos).round(2))\n",
    "\n",
    "    # Gerar e exibir os gráficos\n",
    "    gerar_graficos(df)\n",
    "else:\n",
    "    print(\"\\nNenhum dado válido foi processado para gerar os gráficos.\")\n",
    "\n",
    "print(\"\\nAnálise finalizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6786c18",
   "metadata": {},
   "source": [
    "Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e609b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas e configurações iniciais definidas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm # Usar tqdm.notebook para melhor integração com Jupyter\n",
    "\n",
    "# --- Configurações Iniciais ---\n",
    "\n",
    "# Caminho base onde os repositórios minerados estão localizados\n",
    "BASE_PATH = \"repositories-mined\"\n",
    "\n",
    "# Define um estilo global e tamanho padrão para os gráficos\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1, rc={\"figure.figsize\": (12, 7)})\n",
    "\n",
    "print(\"Bibliotecas importadas e configurações iniciais definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ae730",
   "metadata": {},
   "source": [
    " Carregamento e Enriquecimento dos Dados (Processo Pesado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba9ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando varredura em 'repositories-mined'...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m repo_list \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(BASE_PATH) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, d))]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Percorre cada repositório\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessando repositórios\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     11\u001b[0m     repo_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, repo)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Lê os autores do arquivo de amostra\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Lista para armazenar todos os dados de Pull Requests\n",
    "todos_os_prs = []\n",
    "\n",
    "print(f\"Iniciando varredura em '{BASE_PATH}'...\")\n",
    "\n",
    "# Lista de repositórios para iterar com uma barra de progresso\n",
    "repo_list = [d for d in os.listdir(BASE_PATH) if os.path.isdir(os.path.join(BASE_PATH, d))]\n",
    "\n",
    "# Percorre cada repositório\n",
    "for repo in tqdm(repo_list, desc=\"Processando repositórios\"):\n",
    "    repo_path = os.path.join(BASE_PATH, repo)\n",
    "\n",
    "    # Lê os autores do arquivo de amostra\n",
    "    sample_path = os.path.join(repo_path, \"sample-devs.jsonl\")\n",
    "    if not os.path.isfile(sample_path):\n",
    "        continue\n",
    "\n",
    "    # Carrega os desenvolvedores da amostra para este repositório\n",
    "    devs_neste_repo = {}\n",
    "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            dev_data = json.loads(line)\n",
    "            devs_neste_repo[dev_data['author'].lower()] = dev_data['faixa']\n",
    "\n",
    "    # Percorre a pasta de desenvolvedores para encontrar os PRs\n",
    "    devs_folder_path = os.path.join(repo_path, \"developer\")\n",
    "    if not os.path.isdir(devs_folder_path):\n",
    "        continue\n",
    "\n",
    "    for dev_name in os.listdir(devs_folder_path):\n",
    "        # Verifica se este desenvolvedor está na nossa amostra\n",
    "        if dev_name.lower() in devs_neste_repo:\n",
    "            faixa = devs_neste_repo[dev_name.lower()]\n",
    "            results_path = os.path.join(devs_folder_path, dev_name, \"results\")\n",
    "\n",
    "            if not os.path.isdir(results_path):\n",
    "                continue\n",
    "\n",
    "            for arquivo_json in os.listdir(results_path):\n",
    "                if not arquivo_json.endswith(\".json\"):\n",
    "                    continue\n",
    "                \n",
    "                caminho_arquivo = os.path.join(results_path, arquivo_json)\n",
    "                try:\n",
    "                    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "                        pr = json.load(f)\n",
    "\n",
    "                    # Adiciona informações extras que serão úteis na análise\n",
    "                    pr['author'] = dev_name\n",
    "                    pr['faixa'] = faixa\n",
    "                    pr['repo'] = repo\n",
    "                    todos_os_prs.append(pr)\n",
    "                except Exception as e:\n",
    "                    print(f\"Aviso: Erro ao ler o arquivo {caminho_arquivo}: {e}\")\n",
    "\n",
    "print(f\"\\nCarregamento concluído. Total de PRs coletados: {len(todos_os_prs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4ef47",
   "metadata": {},
   "source": [
    "Pré-processamento e Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc0a9a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Converter colunas de data para o formato datetime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# O argumento errors='coerce' transforma datas inválidas em NaT (Not a Time)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_at\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_at\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Garantir que a faixa do desenvolvedor seja tratada como uma categoria\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Converter colunas de data para o formato datetime\n",
    "# O argumento errors='coerce' transforma datas inválidas em NaT (Not a Time)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "df['merged_at'] = pd.to_datetime(df['merged_at'], errors='coerce')\n",
    "\n",
    "# Garantir que a faixa do desenvolvedor seja tratada como uma categoria\n",
    "df['faixa_desenvolvedor'] = df['faixa_desenvolvedor'].astype('category')\n",
    "\n",
    "# Verificar se a conversão foi bem-sucedida\n",
    "print(\"Tipos de dados após a conversão:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa190133",
   "metadata": {},
   "source": [
    "Definição das Funções de Análise e Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_eficiencia(prs):\n",
    "    \"\"\"\n",
    "    Processa a lista de PRs brutos e extrai métricas de eficiência.\n",
    "    \"\"\"\n",
    "    registros = []\n",
    "    for pr in tqdm(prs, desc=\"Processando PRs para análise\"):\n",
    "        try:\n",
    "            created_at = datetime.fromisoformat(pr['created_at'].replace(\"Z\", \"+00:00\"))\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "\n",
    "        merged_at = pr.get(\"merged_at\")\n",
    "        tempo_merge = None\n",
    "        if merged_at:\n",
    "            try:\n",
    "                merged = datetime.fromisoformat(merged_at.replace(\"Z\", \"+00:00\"))\n",
    "                tempo_merge = (merged - created_at).total_seconds() / 3600\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "\n",
    "        ci_status = pr.get(\"ci_status_on_head\", \"unknown\")\n",
    "        ci_sucesso = 1 if ci_status == \"success\" else 0\n",
    "        ci_valido = ci_status in [\"success\", \"failure\", \"pending\"]\n",
    "\n",
    "        registros.append({\n",
    "            \"faixa\": pr[\"faixa\"], \"tempo_merge_horas\": tempo_merge,\n",
    "            \"ci_status\": ci_status, \"ci_sucesso\": ci_sucesso, \"ci_valido\": ci_valido,\n",
    "            \"hora_criacao\": created_at.hour, \"dia_semana_criacao\": created_at.weekday(),\n",
    "        })\n",
    "    return pd.DataFrame(registros)\n",
    "\n",
    "def gerar_graficos(df, ordem, paleta):\n",
    "    \"\"\"\n",
    "    Gera e exibe os gráficos da análise de eficiência.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Gerando Gráficos de Análise de Eficiência ---\")\n",
    "\n",
    "    # Gráfico 1: Tempo médio para merge\n",
    "    plt.figure()\n",
    "    sns.barplot(data=df, x='faixa', y='tempo_merge_horas', order=ordem, errorbar='sd', palette=paleta, capsize=.2)\n",
    "    plt.title(\"Tempo Médio para Merge por Faixa (com Desvio Padrão)\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Tempo para Merge (em horas)\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfico 2: Taxa de sucesso da CI\n",
    "    plt.figure()\n",
    "    df_ci = df[df[\"ci_valido\"]].groupby(\"faixa\")[\"ci_sucesso\"].mean().reindex(ordem)\n",
    "    sns.barplot(x=df_ci.index, y=df_ci.values, palette=paleta)\n",
    "    plt.title(\"Taxa de Sucesso da Integração Contínua (CI) por Faixa\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Taxa de Sucesso (%)\", fontsize=12)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfico 3: Distribuição da Hora de criação\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=df, x='faixa', y='hora_criacao', order=ordem, palette=paleta)\n",
    "    plt.title(\"Distribuição da Hora de Criação de PRs por Faixa\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Hora do Dia (0-23h)\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfico 4: Distribuição do Dia da semana de criação\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=df, x='faixa', y='dia_semana_criacao', order=ordem, palette=paleta)\n",
    "    plt.title(\"Distribuição do Dia da Semana de Criação de PRs por Faixa\", fontsize=16)\n",
    "    plt.xlabel(\"Faixa do Desenvolvedor\", fontsize=12)\n",
    "    plt.ylabel(\"Dia da Semana\", fontsize=12)\n",
    "    plt.yticks(ticks=range(7), labels=[\"Segunda\", \"Terça\", \"Quarta\", \"Quinta\", \"Sexta\", \"Sábado\", \"Domingo\"])\n",
    "    plt.show()\n",
    "\n",
    "print(\"Funções de análise e visualização definidas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
